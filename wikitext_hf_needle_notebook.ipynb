{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiText-103 with Hugging Face + Needle\n",
    "\n",
    "This notebook does the following:\n",
    "\n",
    "1. Uses **Hugging Face `datasets`** to download WikiText-103 (`wikitext-103-v1`).\n",
    "2. Writes the splits into `wiki.train.tokens`, `wiki.valid.tokens`, `wiki.test.tokens` in a local folder.\n",
    "3. Uses your existing **`needle.data.datasets.wikitext_dataset`** `Corpus` + `batchify` utilities.\n",
    "4. Trains and evaluates a language model using your **`train_wikitext`** and **`evaluate_wikitext`** functions from `apps/simple_ml.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, sys\n",
    "import numpy as np\n",
    "\n",
    "# Make `python/` visible as a package root\n",
    "sys.path.append(\"python\")\n",
    "\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "from needle import Tensor\n",
    "\n",
    "# Training / evaluation helpers from your homework\n",
    "from apps.simple_ml import train_wikitext, evaluate_wikitext\n",
    "\n",
    "# Your language model definition (adjust class / args as needed)\n",
    "from apps.models import LanguageModel  # change if your LM class has a different name\n",
    "\n",
    "# Your WikiText dataset helpers\n",
    "from needle.data.datasets import wikitext_dataset as wt\n",
    "\n",
    "device = ndl.cpu()  # or ndl.cuda() if you wired up a GPU backend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download WikiText-103 using Hugging Face `datasets`\n",
    "\n",
    "This defines a small helper that\n",
    "\n",
    "- calls `load_dataset(\"wikitext\", \"wikitext-103-v1\")`\n",
    "- writes `wiki.train.tokens`, `wiki.valid.tokens`, `wiki.test.tokens`\n",
    "  into `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wikitext103_hf(data_dir: str = \"./wikitext-103\", overwrite: bool = False) -> str:\n",
    "    \"\"\"Download WikiText-103 via Hugging Face `datasets`.\n",
    "\n",
    "    Creates three files in `data_dir`:\n",
    "        - wiki.train.tokens\n",
    "        - wiki.valid.tokens\n",
    "        - wiki.test.tokens\n",
    "\n",
    "    Returns:\n",
    "        data_dir (str): directory containing the .tokens files.\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    train_f = os.path.join(data_dir, \"wiki.train.tokens\")\n",
    "    valid_f = os.path.join(data_dir, \"wiki.valid.tokens\")\n",
    "    test_f  = os.path.join(data_dir, \"wiki.test.tokens\")\n",
    "\n",
    "    if (not overwrite\n",
    "        and os.path.exists(train_f)\n",
    "        and os.path.exists(valid_f)\n",
    "        and os.path.exists(test_f)):\n",
    "        print(f\"[wikitext] Files already exist in {data_dir}, skipping download.\")\n",
    "        return data_dir\n",
    "\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\n",
    "            \"Hugging Face `datasets` is not installed. \"\n",
    "            \"Install it with `pip install datasets`.\"\n",
    "        ) from e\n",
    "\n",
    "    print(\"[wikitext] Downloading WikiText-103 via Hugging Face datasets...\")\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "    def _write_split(split_name: str, out_path: str):\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in ds[split_name]:\n",
    "                # HF can give None for empty lines\n",
    "                text = row[\"text\"] if row[\"text\"] is not None else \"\"\n",
    "                f.write(text.rstrip() + \"\\n\")\n",
    "\n",
    "    _write_split(\"train\", train_f)\n",
    "    _write_split(\"validation\", valid_f)\n",
    "    _write_split(\"test\", test_f)\n",
    "\n",
    "    print(f\"[wikitext] Saved splits to {data_dir}\")\n",
    "    return data_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Corpus` and batchify\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Call the downloader (only downloads the first time).\n",
    "2. Use `wt.Corpus` + `wt.batchify` to get language-model training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where .tokens files will live\n",
    "data_dir = \"./wikitext-103\"  # you can change this\n",
    "\n",
    "# 1) Download (does nothing if files already exist and overwrite=False)\n",
    "download_wikitext103_hf(data_dir, overwrite=False)\n",
    "\n",
    "# 2) Build Corpus\n",
    "# NOTE: adjust use_subword / vocab size to match your wikitext_dataset implementation.\n",
    "corpus = wt.Corpus(\n",
    "    data_dir,\n",
    "    max_lines=None,          # set to a small int to debug on fewer lines\n",
    "    use_subword=False,       # True if you added BPE/subword support there\n",
    ")\n",
    "\n",
    "vocab_size = corpus.vocab_size\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = wt.batchify(corpus.train, batch_size, device=device, dtype=\"float32\")\n",
    "valid_data = wt.batchify(corpus.valid, batch_size, device=device, dtype=\"float32\")\n",
    "test_data  = wt.batchify(corpus.test,  batch_size, device=device, dtype=\"float32\")\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Valid data shape:\", valid_data.shape)\n",
    "print(\"Test  data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model\n",
    "\n",
    "We have a `LanguageModel` class in `apps/models.py` taking\n",
    "`vocab_size`, `embedding_size`, `hidden_size`, `num_layers`, `device`, `dtype`.\n",
    "\n",
    "Change the constructor / class name if your implementation differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    device=device,\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on WikiText-103\n",
    "\n",
    "Call `train_wikitext` function from `apps/simple_ml.py`.\n",
    "Feel free to tweak `n_epochs`, `lr`, optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 40         # BPTT length\n",
    "n_epochs = 5\n",
    "learning_rate = 4.0\n",
    "weight_decay = 0.0\n",
    "clip = 0.25\n",
    "\n",
    "start_time = time.time()\n",
    "train_acc, train_loss = train_wikitext(\n",
    "    model,\n",
    "    train_data,\n",
    "    seq_len=seq_len,\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=ndl.optim.SGD,   # or ndl.optim.Adam\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    loss_fn=nn.SoftmaxLoss,\n",
    "    clip=clip,\n",
    "    device=device,\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Final train loss: {train_loss:.4f}, train acc: {train_acc:.4f}\")\n",
    "print(f\"Train perplexity: {math.exp(train_loss):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on validation and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, val_loss = evaluate_wikitext(\n",
    "    model,\n",
    "    valid_data,\n",
    "    seq_len=seq_len,\n",
    "    loss_fn=nn.SoftmaxLoss,\n",
    "    device=device,\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "print(f\"Valid loss: {val_loss:.4f}, valid acc: {val_acc:.4f}\")\n",
    "print(f\"Valid perplexity: {math.exp(val_loss):.4f}\")\n",
    "\n",
    "test_acc, test_loss = evaluate_wikitext(\n",
    "    model,\n",
    "    test_data,\n",
    "    seq_len=seq_len,\n",
    "    loss_fn=nn.SoftmaxLoss,\n",
    "    device=device,\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "print(f\"Test loss: {test_loss:.4f}, test acc: {test_acc:.4f}\")\n",
    "print(f\"Test perplexity: {math.exp(test_loss):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
